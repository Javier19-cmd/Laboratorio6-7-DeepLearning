{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1sbBXLncHg9BGPt82yxJ0BFtlkjwzS4VV",
      "authorship_tag": "ABX9TyNAElkj0fcvPPKnEU6GXjsD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Javier19-cmd/Laboratorio6-7-DeepLearning/blob/main/Lab67_DL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sistema de recomendación en base al contenido"
      ],
      "metadata": {
        "id": "ewy8dGGtWu3T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import linear_kernel\n",
        "import os\n",
        "import tensorflow as tf\n",
        "\n",
        "# Configurando la TPU en Google Colab\n",
        "resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])\n",
        "tf.config.experimental_connect_to_cluster(resolver)\n",
        "tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "strategy = tf.distribute.experimental.TPUStrategy(resolver)\n",
        "\n",
        "with strategy.scope():\n",
        "    # 1. Cargando de datos y preprocesamiento (se está usando el 10% de los datos)\n",
        "    data = pd.read_csv('/content/drive/MyDrive/Lab67DL/Books.csv')\n",
        "    data = data.sample(frac=0.1, random_state=1).reset_index(drop=True)  # Usando el 10% de los datos\n",
        "\n",
        "    # 2. Creación de perfiles de contenido\n",
        "    data['Content'] = data['Book-Title'] + ' ' + data['Book-Author'] + ' ' + data['Year-Of-Publication'].astype(str) + ' ' + data['Publisher']\n",
        "    data['Content'].fillna('ValorPredeterminado', inplace=True)\n",
        "\n",
        "    print(data[\"Book-Title\"])\n",
        "\n",
        "    # 3. Vectorizando el texto (TF-IDF)\n",
        "    tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
        "    tfidf_matrix = tfidf_vectorizer.fit_transform(data['Content'])\n",
        "\n",
        "    # 4. Calculando la similitud del texto (similitud coseno)\n",
        "    cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)\n",
        "\n",
        "    # 5. Generación de recomendaciones\n",
        "    def get_recommendations(book_title, cosine_sim=cosine_sim):\n",
        "        if book_title in data['Book-Title'].values:\n",
        "            idx = data[data['Book-Title'] == book_title].index[0]  # Obtiene el índice del primer libro coincidente en el nuevo conjunto de datos\n",
        "            if idx < len(cosine_sim):  # Asegúrate de que el índice esté dentro del rango de la matriz de similitud del coseno\n",
        "                sim_scores = list(enumerate(cosine_sim[idx]))\n",
        "                sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
        "                sim_scores = sim_scores[1:11]  # Recomienda los 10 libros más similares (excluyendo el libro de entrada)\n",
        "                book_indices = [i[0] for i in sim_scores]\n",
        "                return data['Book-Title'].iloc[book_indices]\n",
        "            else:\n",
        "                return pd.Series([])  # Devuelve una Serie vacía si el índice está fuera del rango de la matriz de similitud del coseno\n",
        "        else:\n",
        "            return pd.Series([])  # Devuelve una Serie vacía si el libro no se encuentra en el conjunto de datos"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZdbSc3PTXh93",
        "outputId": "038114ee-3554-4adb-b655-5ce32a54a88d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`tf.distribute.experimental.TPUStrategy` is deprecated, please use  the non experimental symbol `tf.distribute.TPUStrategy` instead.\n",
            "<ipython-input-1-7fbded33031b>:15: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  data = pd.read_csv('/content/drive/MyDrive/Lab67DL/Books.csv')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0                                       LOST IN THE MUSEUM\n",
            "1                                   Skinned Alive: Stories\n",
            "2         Silly Mid-off: How to Survive the Cricket Season\n",
            "3                        El Pianista del Gueto de Varsovia\n",
            "4        Merry Wives of Windsor (Arden Shakespeare Seco...\n",
            "                               ...                        \n",
            "27131    Valley of Horses (Thorndike Large Print Basic ...\n",
            "27132    An Elizabethan Progress: The Queen's Journey i...\n",
            "27133    Millionaire'S Pregnant Bride (Texas Cattleman'...\n",
            "27134                                             Rosewood\n",
            "27135     Data &amp; Computer Communications (6th Edition)\n",
            "Name: Book-Title, Length: 27136, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "libro = \"Skinned Alive: Stories\"\n",
        "\n",
        "if libro in data['Book-Title'].values:\n",
        "    recommendations = get_recommendations(libro)\n",
        "    print(\"La recomendación es:\")\n",
        "    print(recommendations)\n",
        "else:\n",
        "  print(f\"'{libro}' no se encuentra en el conjunto de datos.\")"
      ],
      "metadata": {
        "id": "Ao2bQFrk27YB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14f00768-e127-47d8-fc13-f8f0623a3081"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "La recomendación es:\n",
            "4788                      Fanny: A Fiction (White, Edmund)\n",
            "17816                                     Forgetting Elena\n",
            "5008                                  Sketches from Memory\n",
            "6696                    The Enchanted Land (Romance Alive)\n",
            "26573      To the Wedding: A Novel (Vintage International)\n",
            "19116                                          Another You\n",
            "470                        Our Paris: Sketches from Memory\n",
            "8533          River Dogs: Stories (Vintage Contemporaries)\n",
            "18308    The Safety of Objects: Stories (Vintage Contem...\n",
            "10586                      Nightrose (Romance Alive Audio)\n",
            "Name: Book-Title, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sistema de recomendación en base al filtro colaborativo"
      ],
      "metadata": {
        "id": "-venuwhpWnkZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "NNRM"
      ],
      "metadata": {
        "id": "UBfZwg9sPhr9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from keras.layers import Input, Embedding, Flatten, Concatenate, Dense\n",
        "from keras.models import Model\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Configurar la TPU en Google Colab\n",
        "resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])\n",
        "tf.config.experimental_connect_to_cluster(resolver)\n",
        "tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "strategy = tf.distribute.experimental.TPUStrategy(resolver)\n",
        "\n",
        "with strategy.scope():\n",
        "    # Carga los datos\n",
        "    users_data = pd.read_csv('/content/drive/MyDrive/Lab67DL/Users.csv')\n",
        "    books_data = pd.read_csv('/content/drive/MyDrive/Lab67DL/Books.csv')\n",
        "    ratings_data = pd.read_csv('/content/drive/MyDrive/Lab67DL/Ratings.csv')\n",
        "\n",
        "    users_data = users_data.sample(frac=0.1, random_state=1).reset_index(drop=True)\n",
        "    books_data = books_data.sample(frac=0.1, random_state=1).reset_index(drop=True)\n",
        "    ratings_data = ratings_data.sample(frac=0.1, random_state=1).reset_index(drop=True)\n",
        "\n",
        "    ratings_data['ISBN'] = ratings_data['ISBN'].str.rstrip('X')\n",
        "\n",
        "    label_encoder = LabelEncoder()\n",
        "    ratings_data['ISBN'] = label_encoder.fit_transform(ratings_data['ISBN'])\n",
        "\n",
        "    # Definir las incrustaciones de usuario y libro\n",
        "    num_users = len(users_data) + 1\n",
        "    num_books = len(books_data) + 1\n",
        "    embedding_dim = 200\n",
        "\n",
        "    user_input = Input(shape=[1], name=\"User-Input\")\n",
        "    user_embedding = Embedding(num_users, embedding_dim, name=\"User-Embedding\")(user_input)\n",
        "    user_vec = Flatten(name=\"Flatten-Users\")(user_embedding)\n",
        "\n",
        "    book_input = Input(shape=[1], name=\"Book-Input\")\n",
        "    book_embedding = Embedding(num_books, embedding_dim, name=\"Book-Embedding\")(book_input)\n",
        "    book_vec = Flatten(name=\"Flatten-Books\")(book_embedding)\n",
        "\n",
        "    # Concatenar las incrustaciones de usuario y libro\n",
        "    concat = Concatenate()([user_vec, book_vec])\n",
        "\n",
        "    # Capas densas para la predicción de calificación\n",
        "    dense1 = Dense(128, activation='relu')(concat)\n",
        "    dense2 = Dense(64, activation='relu')(dense1)\n",
        "    output = Dense(1, activation='linear')(dense2)\n",
        "\n",
        "    model = Model(inputs=[user_input, book_input], outputs=output)\n",
        "    model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mean_squared_error'])\n",
        "\n",
        "    history = model.fit([ratings_data['User-ID'], ratings_data['ISBN']], ratings_data['Book-Rating'], epochs=30, verbose=1)\n",
        "\n",
        "# Hacer predicciones\n",
        "user_id = 123  # ID de usuario deseado\n",
        "book_id = 456  # ID de libro deseado\n",
        "predicted_rating = model.predict([np.array([user_id]), np.array([book_id])])\n",
        "\n",
        "print(f'Predicción de calificación para el usuario {user_id} y el libro {book_id}: {predicted_rating[0][0]}')"
      ],
      "metadata": {
        "id": "sH4b2JiyNTfu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2d1e5cd-5c31-4c80-ab87-d37548dd055f"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:TPU system grpc://10.94.46.226:8470 has already been initialized. Reinitializing the TPU can cause previously created variables on TPU to be lost.\n",
            "WARNING:absl:`tf.distribute.experimental.TPUStrategy` is deprecated, please use  the non experimental symbol `tf.distribute.TPUStrategy` instead.\n",
            "<ipython-input-33-d64af8c29fc4>:17: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  books_data = pd.read_csv('/content/drive/MyDrive/Lab67DL/Books.csv')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "3594/3594 [==============================] - 100s 26ms/step - loss: 14.7617 - mean_squared_error: 14.7617\n",
            "Epoch 2/30\n",
            "3594/3594 [==============================] - 89s 25ms/step - loss: 12.6464 - mean_squared_error: 12.6464\n",
            "Epoch 3/30\n",
            "3594/3594 [==============================] - 86s 24ms/step - loss: 11.8670 - mean_squared_error: 11.8670\n",
            "Epoch 4/30\n",
            "3594/3594 [==============================] - 88s 24ms/step - loss: 11.4138 - mean_squared_error: 11.4138\n",
            "Epoch 5/30\n",
            "3594/3594 [==============================] - 85s 24ms/step - loss: 11.2768 - mean_squared_error: 11.2768\n",
            "Epoch 6/30\n",
            "3594/3594 [==============================] - 84s 23ms/step - loss: 11.1849 - mean_squared_error: 11.1849\n",
            "Epoch 7/30\n",
            "3594/3594 [==============================] - 85s 24ms/step - loss: 11.1247 - mean_squared_error: 11.1247\n",
            "Epoch 8/30\n",
            "3594/3594 [==============================] - 85s 24ms/step - loss: 11.1282 - mean_squared_error: 11.1282\n",
            "Epoch 9/30\n",
            "3594/3594 [==============================] - 85s 24ms/step - loss: 11.0909 - mean_squared_error: 11.0909\n",
            "Epoch 10/30\n",
            "3594/3594 [==============================] - 85s 24ms/step - loss: 11.0141 - mean_squared_error: 11.0141\n",
            "Epoch 11/30\n",
            "3594/3594 [==============================] - 89s 25ms/step - loss: 10.9564 - mean_squared_error: 10.9564\n",
            "Epoch 12/30\n",
            "3594/3594 [==============================] - 87s 24ms/step - loss: 10.9387 - mean_squared_error: 10.9387\n",
            "Epoch 13/30\n",
            "3594/3594 [==============================] - 87s 24ms/step - loss: 10.9109 - mean_squared_error: 10.9109\n",
            "Epoch 14/30\n",
            "3594/3594 [==============================] - 87s 24ms/step - loss: 10.8823 - mean_squared_error: 10.8823\n",
            "Epoch 15/30\n",
            "3594/3594 [==============================] - 87s 24ms/step - loss: 10.8693 - mean_squared_error: 10.8693\n",
            "Epoch 16/30\n",
            "3594/3594 [==============================] - 85s 24ms/step - loss: 10.8581 - mean_squared_error: 10.8581\n",
            "Epoch 17/30\n",
            "3594/3594 [==============================] - 84s 23ms/step - loss: 10.8329 - mean_squared_error: 10.8329\n",
            "Epoch 18/30\n",
            "3594/3594 [==============================] - 85s 24ms/step - loss: 10.8317 - mean_squared_error: 10.8317\n",
            "Epoch 19/30\n",
            "3594/3594 [==============================] - 83s 23ms/step - loss: 10.8199 - mean_squared_error: 10.8199\n",
            "Epoch 20/30\n",
            "3594/3594 [==============================] - 85s 24ms/step - loss: 10.8105 - mean_squared_error: 10.8105\n",
            "Epoch 21/30\n",
            "3594/3594 [==============================] - 84s 23ms/step - loss: 10.8090 - mean_squared_error: 10.8090\n",
            "Epoch 22/30\n",
            "3594/3594 [==============================] - 85s 24ms/step - loss: 10.7986 - mean_squared_error: 10.7986\n",
            "Epoch 23/30\n",
            "3594/3594 [==============================] - 88s 25ms/step - loss: 10.7912 - mean_squared_error: 10.7912\n",
            "Epoch 24/30\n",
            "3594/3594 [==============================] - 86s 24ms/step - loss: 10.7908 - mean_squared_error: 10.7908\n",
            "Epoch 25/30\n",
            "3594/3594 [==============================] - 85s 24ms/step - loss: 10.7822 - mean_squared_error: 10.7822\n",
            "Epoch 26/30\n",
            "3594/3594 [==============================] - 85s 24ms/step - loss: 10.7794 - mean_squared_error: 10.7794\n",
            "Epoch 27/30\n",
            "3594/3594 [==============================] - 85s 24ms/step - loss: 10.7785 - mean_squared_error: 10.7785\n",
            "Epoch 28/30\n",
            "3594/3594 [==============================] - 85s 24ms/step - loss: 10.7700 - mean_squared_error: 10.7700\n",
            "Epoch 29/30\n",
            "3594/3594 [==============================] - 84s 23ms/step - loss: 10.7652 - mean_squared_error: 10.7652\n",
            "Epoch 30/30\n",
            "3594/3594 [==============================] - 86s 24ms/step - loss: 10.7649 - mean_squared_error: 10.7649\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "Predicción de calificación para el usuario 123 y el libro 456: 1.015446662902832\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rlk2MrgaA1gi"
      },
      "outputs": [],
      "source": [
        "# import tensorflow as tf\n",
        "# import pandas as pd\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# from sklearn.preprocessing import LabelEncoder\n",
        "# import os\n",
        "\n",
        "# # Carga los datos\n",
        "# users_data = pd.read_csv('/content/drive/MyDrive/Lab67DL/Users.csv')\n",
        "# books_data = pd.read_csv('/content/drive/MyDrive/Lab67DL/Books.csv')\n",
        "# ratings_data = pd.read_csv('/content/drive/MyDrive/Lab67DL/Ratings.csv')\n",
        "\n",
        "# ratings_data['ISBN'] = ratings_data['ISBN'].str.rstrip('X')\n",
        "\n",
        "# label_encoder = LabelEncoder()\n",
        "# ratings_data['ISBN'] = label_encoder.fit_transform(ratings_data['ISBN'])\n",
        "\n",
        "# # Divide los datos en conjuntos de entrenamiento y prueba\n",
        "# X_train, X_test, y_train, y_test = train_test_split(ratings_data[[\"User-ID\", \"ISBN\"]], ratings_data['Book-Rating'], test_size=0.2, random_state=42)\n",
        "\n",
        "# print(X_train.shape)\n",
        "# print(y_train.shape)\n",
        "\n",
        "# # Convierte el modelo para usar TPU\n",
        "# resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])\n",
        "# tf.config.experimental_connect_to_cluster(resolver)\n",
        "# tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "# strategy = tf.distribute.experimental.TPUStrategy(resolver)\n",
        "\n",
        "# with strategy.scope():\n",
        "#     model = tf.keras.Sequential([\n",
        "#         tf.keras.layers.Embedding(input_dim=len(ratings_data['ISBN'].unique()) + 1, output_dim=64),\n",
        "#         tf.keras.layers.Dense(256, activation='relu'),  # Capa oculta con 256 neuronas\n",
        "#         tf.keras.layers.Dense(128, activation='relu'),  # Capa oculta con 128 neuronas\n",
        "#         tf.keras.layers.Dense(64, activation='relu'),   # Otra capa oculta con 64 neuronas\n",
        "#         tf.keras.layers.Dense(32, activation='relu'),   # Capa oculta con 32 neuronas\n",
        "#         tf.keras.layers.Dense(16, activation='relu'),   # Capa oculta con 16 neuronas\n",
        "#         tf.keras.layers.Dense(8, activation='relu'),    # Capa oculta con 8 neuronas\n",
        "#         tf.keras.layers.Dense(1)\n",
        "#     ])\n",
        "\n",
        "\n",
        "\n",
        "#     # Compila el modelo\n",
        "#     model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "\n",
        "# print(model.summary())\n",
        "\n",
        "# with strategy.scope():\n",
        "\n",
        "#   # Entrena el modelo\n",
        "#   model.fit(X_train, y_train, epochs=50, batch_size=256, validation_split=0.2)"
      ]
    }
  ]
}